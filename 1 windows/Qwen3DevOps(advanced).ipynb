{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002430b5",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e7890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Processing dates:   1%|          | 1/182 [00:44<2:14:17, 44.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing dates: 100%|██████████| 182/182 [2:04:02<00:00, 40.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num samples generated: 1274, Total num abnormalities: 244 , Num abnormality rate: 19.15%\n",
      "Total text samples generated: 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "model_name = 'Qwen/Qwen3-0.6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Example data\n",
    "template_data = {\n",
    "    \"date\": [f\"2024-05-{str(day).zfill(2)}\" for day in range(1, 32)],\n",
    "    \"hivenode01_memory_usage\": [0.383553, 0.418137, 0.492768, 0.565933, 0.563794, 0.530966, 0.542266, 0.49038, 0.495031, 0.563613, \n",
    "                                0.495269, 0.618042, 0.536223, 0.613919, 0.508449, 0.576679, 0.521551, 0.622268, 0.611154, 0.518589, \n",
    "                                0.654723, 0.529004, 0.634645, 0.537238, 0.534131, 0.536508, 0.543266, 0.575944, 0.53557, 0.611579, 0.551645],\n",
    "    \"hivenode01_cpu_load_5min\": [8.53, 12.07, 9.09, 8.55, 12.5, 7.98, 8.93, 7.85, 7.22, 8.98, \n",
    "                                 9.34, 9.6, 10.06, 8.68, 9.03, 8.08, 9.02, 7.89, 7.6, 11.22, \n",
    "                                 9.58, 9.53, 8.96, 10.89, 6.7, 9.73, 9.5, 10.5, 9.11, 8.75, 6.51],\n",
    "    \"hivenode01_cpu_load_10min\": [9.12, 9.94, 9.3, 8.8, 11.66, 9.5, 8.63, 8.67, 7.77, 8.18, \n",
    "                                  8.99, 8.14, 9.02, 8.62, 8.54, 8.71, 8.91, 8.47, 7.66, 9.91, \n",
    "                                  8.67, 8.72, 8.77, 9.19, 7.58, 9.44, 8.89, 9.83, 8.32, 8.91, 8.17],\n",
    "    \"hivenode01_cpu_load_15min\": [9.49, 9.07, 9.6, 7.92, 9.79, 9.62, 8.36, 9.32, 8.07, 7.91, \n",
    "                                  8.99, 8.12, 8.4, 8.46, 8.25, 8.73, 9.04, 8.54, 8.02, 9.58, \n",
    "                                  8.62, 8.84, 8.31, 8.68, 8.11, 9.01, 8.3, 8.95, 8.64, 8.83, 8.73],\n",
    "    \"hivenode02_memory_usage\": [0.342719, 0.374206, 0.404097, 0.427319, 0.435087, 0.43736, 0.437884, 0.438484, 0.447014, 0.450261, \n",
    "                                0.456589, 0.457304, 0.461043, 0.459707, 0.456962, 0.460743, 0.470204, 0.466149, 0.474047, 0.476227, \n",
    "                                0.477294, 0.479754, 0.482177, 0.471509, 0.475243, 0.47675, 0.487594, 0.490805, 0.492136, 0.500023, 0.502623],\n",
    "    \"hivenode02_cpu_load_5min\": [6.51, 8.39, 10.08, 12.8, 12.48, 10.14, 12.26, 8.85, 5.73, 9.06, \n",
    "                                 7.78, 7.96, 7.47, 6.82, 5.79, 7.28, 6.23, 7.19, 6.14, 7.79, \n",
    "                                 6.3, 6.57, 7.69, 6.68, 6, 6.59, 7.59, 9.85, 7.96, 9.24, 7.76],\n",
    "    \"hivenode02_cpu_load_10min\": [6.27, 8.1, 10.61, 11.74, 12.44, 10.47, 11.24, 9.63, 7.08, 8.45, \n",
    "                                  7.22, 6.18, 6.94, 7, 6.41, 7.03, 6.72, 6.58, 6.04, 8.39, \n",
    "                                  6.34, 6.72, 6.91, 6.78, 6.35, 6.98, 6.84, 9.35, 8.14, 9.07, 7.54],\n",
    "    \"hivenode02_cpu_load_15min\": [6.74, 7.63, 10.64, 10.35, 11.11, 10.6, 10.01, 9.69, 7.04, 8.02, \n",
    "                                  6.8, 6.12, 6.77, 7.13, 6.7, 7.07, 7.05, 6.9, 6.66, 8.08, \n",
    "                                  7, 7.39, 6.79, 7.14, 6.82, 7.05, 7.04, 8.52, 8, 8.46, 7.58],\n",
    "    \"hivenode03_memory_usage\": [0.348359, 0.375609, 0.40562, 0.428764, 0.439686, 0.441799, 0.445848, 0.442726, 0.455341, 0.451364, \n",
    "                                0.458707, 0.458583, 0.463021, 0.474306, 0.468443, 0.46892, 0.47674, 0.473441, 0.477693, 0.477164, \n",
    "                                0.478697, 0.480722, 0.4821, 0.486745, 0.489345, 0.488128, 0.493571, 0.493586, 0.494228, 0.502349, 0.503208],\n",
    "    \"hivenode03_cpu_load_5min\": [8.1, 9.61, 7.42, 8.05, 10.37, 7.72, 11.72, 7.67, 6.14, 9.78, \n",
    "                                 7.17, 7.31, 10.6, 6.07, 5.34, 8.05, 6.25, 6.13, 6.1, 8.68, \n",
    "                                 10.09, 7.21, 7.5, 7.04, 7.07, 8.59, 10.04, 7.41, 6.68, 12.27, 6.57],\n",
    "    \"hivenode03_cpu_load_10min\": [7.6, 8.99, 8.52, 8.7, 10.56, 8.82, 9.46, 7.53, 7.36, 8.75, \n",
    "                                  7.09, 7.13, 9.56, 6.75, 6.73, 7.91, 6.71, 6.66, 7.11, 8.29, \n",
    "                                  8.12, 7, 6.84, 7.52, 7.84, 8.83, 8.43, 8.22, 7.42, 8.92, 7.05],\n",
    "    \"hivenode03_cpu_load_15min\": [7.77, 8.52, 8.85, 8.12, 9.56, 9.05, 8.9, 8.19, 7.57, 8.34, \n",
    "                                  7.3, 7.36, 8.66, 7.17, 7.57, 7.68, 7.33, 7.05, 7.46, 8.48, \n",
    "                                  7.77, 7.61, 7.16, 7.84, 8.22, 8.38, 8.05, 8.11, 7.66, 8.29, 7.49],\n",
    "    \"hivenode04_memory_usage\": [0.406619, 0.419903, 0.433678, 0.441757, 0.452172, 0.455543, 0.4598, 0.460577, 0.459536, 0.461846, \n",
    "                                0.465693, 0.474606, 0.478739, 0.470002, 0.465657, 0.467247, 0.473648, 0.473736, 0.478283, 0.482861, \n",
    "                                0.483788, 0.488319, 0.489811, 0.485238, 0.493503, 0.487962, 0.496605, 0.49354, 0.490805, 0.485683, 0.487579],\n",
    "    \"hivenode04_cpu_load_5min\": [8, 10.23, 11.02, 7.19, 14.19, 9.73, 8, 7.59, 5.16, 7.4, \n",
    "                                 6.75, 11.49, 7.46, 6.14, 5.92, 8.56, 8.31, 5.56, 5.3, 8.13, \n",
    "                                 9.03, 7.09, 6.19, 9.83, 7.2, 6.55, 8.41, 8.59, 8.73, 7.54, 8.48],\n",
    "    \"hivenode04_cpu_load_10min\": [6.98, 9.55, 9.47, 6.98, 10.99, 9.18, 7.47, 8.12, 6.7, 7.94, \n",
    "                                  6.55, 8.76, 7.3, 6.44, 7.34, 8.53, 8.74, 5.8, 6.15, 7.7, \n",
    "                                  8.45, 7, 6.08, 8.91, 7.49, 6.32, 8.62, 8.64, 9.39, 7.11, 8.12],\n",
    "    \"hivenode04_cpu_load_15min\": [7.1, 8.69, 8.87, 7.09, 9.12, 8.57, 7.35, 8.13, 7.05, 7.85, \n",
    "                                  6.75, 7.59, 7.14, 6.73, 7.68, 8.18, 8.23, 6.5, 6.73, 7.81, \n",
    "                                  7.99, 7.58, 6.5, 8.22, 7.47, 6.78, 7.95, 8.17, 9.28, 7.25, 7.87],\n",
    "    \"hivenode05_memory_usage\": [0.351445, 0.383579, 0.411094, 0.438981, 0.445999, 0.448852, 0.449878, 0.451535, 0.458583, 0.462726, \n",
    "                                0.466429, 0.469966, 0.471406, 0.469826, 0.481328, 0.483001, 0.476077, 0.489852, 0.49036, 0.485502, \n",
    "                                0.491012, 0.492307, 0.493674, 0.49574, 0.50239, 0.502286, 0.507123, 0.507206, 0.507957, 0.515078, 0.519858],\n",
    "    \"hivenode05_cpu_load_5min\": [6.35, 10.45, 7.17, 7.47, 6.62, 7.21, 9.36, 7.46, 6.07, 7.37, \n",
    "                                 7.32, 7.54, 7.96, 7.12, 6.64, 7.61, 9.68, 7.25, 8.29, 9.44, \n",
    "                                 11.2, 7.9, 9.29, 8.73, 10.4, 6.47, 7.19, 11.26, 9.63, 7.66, 9.04],\n",
    "    \"hivenode05_cpu_load_10min\": [6.28, 8.91, 7.31, 7.82, 7.7, 7.96, 7.92, 7.33, 6.2, 8.48, \n",
    "                                  6.69, 7.32, 7.58, 7.44, 6.99, 7.93, 7.84, 7.75, 7.74, 8.82, \n",
    "                                  10.24, 7.76, 7.84, 8.24, 8.31, 7.4, 7.27, 9.23, 8.78, 7.81, 8.21],\n",
    "    \"hivenode05_cpu_load_15min\": [6.8, 8.32, 7.67, 7.49, 7.61, 8.17, 7.36, 7.52, 6.51, 7.85, \n",
    "                                  7.04, 7.15, 7.65, 7.48, 7.28, 7.67, 7.78, 8.08, 7.52, 8.79, \n",
    "                                  9.57, 7.87, 7.51, 8.07, 7.82, 7.67, 7.49, 8.74, 8.16, 8.06, 7.79],\n",
    "    \"hivenode06_memory_usage\": [0.36604, 0.269375, 0.307352, 0.364142, 0.173979, 0.139898, 0.192349, 0.218628, 0.207623, 0.156441, \n",
    "                                0.164164, 0.205858, 0.18286, 0.156721, 0.154286, 0.194161, 0.153307, 0.17226, 0.154831, 0.158268, \n",
    "                                0.206682, 0.174523, 0.213316, 0.193095, 0.21925, 0.20669, 0.219678, 0.257834, 0.30988, 0.199916, 0.160641],\n",
    "    \"hivenode06_cpu_load_5min\": [3.42, 3.33, 2.71, 2.62, 3.36, 3.65, 2.88, 3, 3.22, 3.28, \n",
    "                                 2.28, 3.02, 3.02, 2.89, 3.22, 3.55, 3.16, 3.36, 3.44, 2.8, \n",
    "                                 2.95, 2.28, 3.14, 2.43, 3.47, 2.78, 2.93, 3.51, 2.69, 2.55, 3.2],\n",
    "    \"hivenode06_cpu_load_10min\": [3.09, 3.3, 3.01, 2.83, 3.27, 3.3, 3.22, 2.9, 3.19, 3.34, \n",
    "                                  2.94, 3.04, 3.19, 3.11, 3.57, 3.37, 3.31, 3.19, 3.12, 3.04, \n",
    "                                  3.05, 2.67, 3.22, 2.84, 3.73, 3.18, 3.1, 3.21, 3.08, 3.09, 3.39],\n",
    "    \"hivenode06_cpu_load_15min\": [3.02, 3.19, 3.03, 2.87, 3.27, 3.17, 3.19, 2.94, 3.12, 3.12, \n",
    "                                  3.06, 3.01, 3.18, 3.38, 3.54, 3.31, 3.35, 3.09, 3.03, 3.12, \n",
    "                                  3.01, 2.76, 3.19, 2.87, 3.45, 3.13, 3.14, 3.13, 3.04, 3.22, 3.33],\n",
    "    \"hivenode07_memory_usage\": [0.381104, 0.141959, 0.15592, 0.144891, 0.183825, 0.221047, 0.255345, 0.270316, 0.25585, 0.255843, \n",
    "                                0.186189, 0.215183, 0.259902, 0.310836, 0.185046, 0.190949, 0.17576, 0.179073, 0.207616, 0.205337, \n",
    "                                0.248384, 0.240304, 0.201106, 0.180418, 0.189253, 0.195343, 0.205485, 0.193422, 0.213659, 0.200243, 0.198259],\n",
    "    \"hivenode07_cpu_load_5min\": [2.87, 2.82, 2.36, 2.04, 2.55, 2.15, 2.05, 2.06, 2.34, 2.16, \n",
    "                                 2.96, 2.44, 2.46, 2.43, 2.34, 2.76, 2.37, 2.58, 2.23, 2.6, \n",
    "                                 2.36, 2.35, 2.13, 2.58, 2.24, 2.66, 2.89, 2.58, 2.12, 3.02, 2.59],\n",
    "    \"hivenode07_cpu_load_10min\": [2.41, 2.74, 2.3, 2.23, 2.55, 2.46, 2.54, 2.23, 2.37, 2.36, \n",
    "                                  2.7, 2.6, 2.57, 2.32, 2.67, 2.5, 2.62, 2.64, 2.25, 2.46, \n",
    "                                  2.78, 2.58, 2.36, 2.64, 2.39, 2.79, 2.56, 2.81, 2.43, 2.74, 2.75],\n",
    "    \"hivenode07_cpu_load_15min\": [2.32, 2.75, 2.42, 2.36, 2.45, 2.48, 2.51, 2.27, 2.39, 2.43, \n",
    "                                  2.58, 2.56, 2.54, 2.48, 2.74, 2.52, 2.62, 2.59, 2.43, 2.47, \n",
    "                                  2.71, 2.67, 2.38, 2.56, 2.53, 2.64, 2.42, 2.84, 2.59, 2.66, 2.72]\n",
    "}\n",
    "template_df = pd.DataFrame(template_data)\n",
    "\n",
    "# Date range\n",
    "start_date = datetime(2024, 3, 1)\n",
    "end_date = datetime(2025, 5, 31)\n",
    "\n",
    "# List of nodes\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "# Days in a month\n",
    "def days_in_month(year, month):\n",
    "    if month == 2:\n",
    "        # Check for leap year\n",
    "        if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "            return 29\n",
    "        return 28\n",
    "    return 31 if month in [1, 3, 5, 7, 8, 10, 12] else 30\n",
    "\n",
    "current_date = start_date\n",
    "num_data = []\n",
    "text_data = []\n",
    "total_num_samples = 0\n",
    "total_text_samples = 0\n",
    "total_num_abnormalities = 0\n",
    "total_days = (end_date - start_date).days + 1\n",
    "\n",
    "for _ in tqdm(range(total_days), desc=\"Processing dates\"):\n",
    "    days = days_in_month(current_date.year, current_date.month)\n",
    "\n",
    "    if current_date.day == 1:\n",
    "        text_data = []\n",
    "\n",
    "    template_day_idx = (current_date.day - 1) % 31\n",
    "    template_row = template_df.iloc[template_day_idx]\n",
    "    \n",
    "    for node in tqdm(nodes, desc=\"Processing nodes\", leave=False):\n",
    "        # Use template data directly\n",
    "        memory_usage = template_row[f\"{node}_memory_usage\"]\n",
    "        cpu_load_5min = template_row[f\"{node}_cpu_load_5min\"]\n",
    "        cpu_load_10min = template_row[f\"{node}_cpu_load_10min\"]\n",
    "        cpu_load_15min = template_row[f\"{node}_cpu_load_15min\"]\n",
    "\n",
    "        # Determine status and additional fields\n",
    "        if memory_usage > 0.7 or cpu_load_5min > 10 or cpu_load_10min > 10 or cpu_load_15min > 10:\n",
    "            status = \"abnormal\"\n",
    "            total_num_abnormalities += 1\n",
    "            \n",
    "            # Calculate alert severity\n",
    "            alert_severity = \"CRITICAL\" if memory_usage > 0.9 or cpu_load_5min > 15 else \"HIGH\" if memory_usage > 0.8 or cpu_load_5min > 12 else \"MEDIUM\"\n",
    "            \n",
    "            # Determine likely root cause\n",
    "            likely_root_cause = \"high memory usage\" if memory_usage > 0.7 else \"high CPU load\"\n",
    "            \n",
    "            # Determine impact scope\n",
    "            impact_scope = \"node\" if node == \"hivenode01\" else \"cluster\"\n",
    "            \n",
    "            # Suggest remediation steps\n",
    "            remediation_steps = \"restart node, increase resources, optimize code\"\n",
    "            \n",
    "            prompt = (\n",
    "                f\"You are Qwen3-0.6B, a language model for generating operation logs. \"\n",
    "                f\"Generate a single log entry with these requirements:\\n\"\n",
    "                f\"1. Retain all numerical values unchanged.\\n\"\n",
    "                f\"2. Use ISO 8601 timestamp (to the second).\\n\"\n",
    "                f\"3. Include fields: log_level (INFO/WARN/ERROR), event_type, node_id, memory_usage, cpu_load_5min, cpu_load_10min, cpu_load_15min, status, \"\n",
    "                f\"alert_severity, likely_root_cause, impact_scope, remediation_steps.\\n\"\n",
    "                f\"4. For status=abnormal: highlight issue with severity, cause, impact, remediation.\\n\"\n",
    "                f\"5. Vary syntax, use domain terms (OOM, jitter, throughput), keep entry 50-150 characters.\\n\"\n",
    "                f\"6. Output only the log entry.\\n\\n\"\n",
    "                f\"Input Data:\\n\"\n",
    "                f\"  timestamp: {current_date.strftime('%Y-%m-%dT%H:%M:%SZ')}\\n\"\n",
    "                f\"  node: {node}\\n\"\n",
    "                f\"  memory_usage: {memory_usage:.4f}\\n\"\n",
    "                f\"  cpu_load_5min: {cpu_load_5min:.2f}\\n\"\n",
    "                f\"  cpu_load_10min: {cpu_load_10min:.2f}\\n\"\n",
    "                f\"  cpu_load_15min: {cpu_load_15min:.2f}\\n\"\n",
    "                f\"  status: {status}\\n\"\n",
    "                f\"  alert_severity: {alert_severity}\\n\"\n",
    "                f\"  likely_root_cause: {likely_root_cause}\\n\"\n",
    "                f\"  impact_scope: {impact_scope}\\n\"\n",
    "                f\"  remediation_steps: {remediation_steps}\\n\"\n",
    "            )\n",
    "        else:\n",
    "            status = \"normal\"\n",
    "            prompt = (\n",
    "                f\"You are Qwen3-0.6B, a language model for generating operation logs. \"\n",
    "                f\"Generate a single log entry with these requirements:\\n\"\n",
    "                f\"1. Retain all numerical values unchanged.\\n\"\n",
    "                f\"2. Use ISO 8601 timestamp (to the second).\\n\"\n",
    "                f\"3. Include fields: log_level (INFO/WARN/ERROR), event_type, node_id, memory_usage, cpu_load_5min, cpu_load_10min, cpu_load_15min, status.\\n\"\n",
    "                f\"4. For status=normal: emphasize stability and optimal performance.\\n\"\n",
    "                f\"5. Vary syntax, use domain terms (OOM, jitter, throughput), keep entry 50-150 characters.\\n\"\n",
    "                f\"6. Output only the log entry.\\n\\n\"\n",
    "                f\"Input Data:\\n\"\n",
    "                f\"  timestamp: {current_date.strftime('%Y-%m-%dT%H:%M:%SZ')}\\n\"\n",
    "                f\"  node: {node}\\n\"\n",
    "                f\"  memory_usage: {memory_usage:.4f}\\n\"\n",
    "                f\"  cpu_load_5min: {cpu_load_5min:.2f}\\n\"\n",
    "                f\"  cpu_load_10min: {cpu_load_10min:.2f}\\n\"\n",
    "                f\"  cpu_load_15min: {cpu_load_15min:.2f}\\n\"\n",
    "                f\"  status: {status}\\n\"\n",
    "            )\n",
    "\n",
    "        # Generate text log\n",
    "        generated = generator(prompt, max_new_tokens=150, num_return_sequences=1, do_sample=True, temperature=0.9)\n",
    "        text = generated[0]['generated_text'].strip()\n",
    "        text_data.append({\n",
    "            \"date\": current_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"node\": node,\n",
    "            \"text\": text,\n",
    "        })\n",
    "        total_text_samples += 1\n",
    "\n",
    "    if current_date.day == days or current_date == end_date:\n",
    "        # Save text data\n",
    "        year_month = current_date.strftime(\"%Y-%m\")\n",
    "        os.makedirs('data/DevOpsLogs', exist_ok=True)\n",
    "        with open(f'data/DevOpsLogs/{year_month}.log', 'w', encoding='utf-8') as f:\n",
    "            for record in text_data:\n",
    "                json.dump(record, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "        text_data = []\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(f'Total text samples generated: {total_text_samples}, Total abnormalities: {total_num_abnormalities}, Abnormality rate: {total_num_abnormalities/total_text_samples:.2%}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcea5a",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f48144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging num data: 100%|██████████| 6/6 [00:00<00:00, 705.18it/s]\n",
      "Merging text data: 100%|██████████| 6/6 [00:00<00:00, 703.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical data: train set 1015 samples, abnormalities 33, abnormality rate 3.25%\n",
      "Numerical data: test set 259 samples, abnormalities 8, abnormality rate 3.09%\n",
      "Text data: train set 1019 samples, test set 255 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Merge num data\n",
    "num_files = sorted([f for f in os.listdir('data/pre_train_num') if f.endswith('.csv')])\n",
    "all_num_data = []\n",
    "for file in tqdm(num_files, desc=\"Merging num data\"):\n",
    "    df = pd.read_csv(os.path.join('data/pre_train_num', file))\n",
    "    all_num_data.append(df)\n",
    "num_df = pd.concat(all_num_data, ignore_index=True)\n",
    "num_df['date'] = pd.to_datetime(num_df['date'], format='%Y-%m-%d')\n",
    "num_df = num_df.sort_values('date')\n",
    "num_df.to_csv('data/pre_train_num/num_202412-202505.csv', index=False)\n",
    "\n",
    "# Merge text data\n",
    "text_files = sorted([f for f in os.listdir('data/pre_train_text') if f.endswith('.jsonl')])\n",
    "all_text_data = []\n",
    "for file in tqdm(text_files, desc=\"Merging text data\"):\n",
    "    with open(os.path.join('data/pre_train_text', file), 'r', encoding='utf-8') as f:\n",
    "        records = [json.loads(line) for line in f]\n",
    "        all_text_data.extend(records)\n",
    "text_df = pd.DataFrame(all_text_data)\n",
    "text_df['date'] = pd.to_datetime(text_df['date'], format='%Y-%m-%d')\n",
    "text_df = text_df.sort_values('date')\n",
    "# Save text data as JSONL\n",
    "with open('data/pre_train_text/text_202412-202505.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for _, row in text_df.iterrows():\n",
    "        record = row.to_dict()\n",
    "        if isinstance(record['date'], pd.Timestamp):\n",
    "            record['date'] = record['date'].strftime('%Y-%m-%d')\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Text train-test split (80/20)\n",
    "split_idx = int(len(num_df) * 0.8)\n",
    "train_num_df = num_df.iloc[:split_idx]\n",
    "test_num_df = num_df.iloc[split_idx:]\n",
    "\n",
    "split_idx_text = int(len(text_df) * 0.8)\n",
    "train_text_df = text_df.iloc[:split_idx_text]\n",
    "test_text_df = text_df.iloc[split_idx_text:]\n",
    "\n",
    "\n",
    "# Save datasets as Parquet to reduce storage space\n",
    "os.makedirs('data/traindata', exist_ok=True)\n",
    "os.makedirs('data/testdata', exist_ok=True)\n",
    "train_num_df.to_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df.to_parquet('data/testdata/test_num.parquet')\n",
    "train_text_df.to_parquet('data/traindata/train_text.parquet')\n",
    "test_text_df.to_parquet('data/testdata/test_text.parquet')\n",
    "\n",
    "# Calculate abnormality for numerical data\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "def count_abnormalities(df):\n",
    "    total_samples = len(df) * len(nodes)\n",
    "    total_abnormal = 0\n",
    "    for node in nodes:\n",
    "        abnormal = ((df[f\"{node}_memory_usage\"] > 0.7) | \n",
    "                    (df[f\"{node}_cpu_load_5min\"] > 10) | \n",
    "                    (df[f\"{node}_cpu_load_10min\"] > 10) | \n",
    "                    (df[f\"{node}_cpu_load_15min\"] > 10)).sum()\n",
    "        total_abnormal += abnormal\n",
    "    return total_samples, total_abnormal\n",
    "\n",
    "# Numerical data statistics\n",
    "train_num_samples, train_num_abnormal = count_abnormalities(train_num_df)\n",
    "test_num_samples, test_num_abnormal = count_abnormalities(test_num_df)\n",
    "train_abn_rate = train_num_abnormal / train_num_samples \n",
    "test_abn_rate = test_num_abnormal / test_num_samples \n",
    "\n",
    "print(f\"Numerical data: train set {train_num_samples} samples, abnormalities {train_num_abnormal}, abnormality rate {train_abn_rate:.2%}\")\n",
    "print(f\"Numerical data: test set {test_num_samples} samples, abnormalities {test_num_abnormal}, abnormality rate {test_abn_rate:.2%}\")\n",
    "print(f\"Text data: train set {train_text_df.shape[0]} samples, test set {test_text_df.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219c555",
   "metadata": {},
   "source": [
    "# LLM & Fine-Tunning -- Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a1710a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution: {0: 1019}\n",
      "Test label distribution: {0: 255}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\accelerate\\utils\\modeling.py:808: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 146 unique dates, 143 dates after 3 days, 7 nodes\n",
      "Dataset: 37 unique dates, 34 dates after 3 days, 7 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 95/189 2:36:54 < 2:38:35, 0.01 it/s, Epoch 1.50/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    145\u001b[39m training_args = TrainingArguments(\n\u001b[32m    146\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mmodels/qwen3-0.6B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    147\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m     eval_accumulation_steps=\u001b[32m1\u001b[39m\n\u001b[32m    164\u001b[39m )\n\u001b[32m    166\u001b[39m trainer = Trainer(\n\u001b[32m    167\u001b[39m     model=model,\n\u001b[32m    168\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m    173\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33mmodels/best-qwen3-0.6B\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmodels/best-qwen3-0.6B\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\transformers\\trainer.py:3791\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3793\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\accelerate\\accelerator.py:2469\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2468\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2469\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2471\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from  torch.cuda.amp import autocast\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.makedirs(\"models/qwen3-0.6B\", exist_ok=True)\n",
    "os.makedirs(\"models/best-qwen3-0.6B\", exist_ok=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_text_df = pd.read_parquet('data/traindata/train_text.parquet')\n",
    "test_text_df = pd.read_parquet('data/testdata/test_text.parquet')\n",
    "\n",
    "def extract_status(text):\n",
    "    match = re.search(r\"(normal|abnormal)\", str(text), re.IGNORECASE)\n",
    "    return 0 if match and match.group(0).lower() == \"normal\" else 1\n",
    "\n",
    "train_text_df['label'] = train_text_df['text'].apply(extract_status)\n",
    "test_text_df['label'] = test_text_df['text'].apply(extract_status)\n",
    "print(\"Train label distribution:\", train_text_df['label'].value_counts().to_dict())\n",
    "print(\"Test label distribution:\", test_text_df['label'].value_counts().to_dict())\n",
    "\n",
    "model_name = 'Qwen/Qwen3-0.6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_type=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False}) \n",
    "\n",
    "#  LoRA(low-Rank Adaptation) configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "model.train()\n",
    "\n",
    "def prepare_history_context(df, date, node, window_days=3):\n",
    "    start_date = pd.to_datetime(date) - pd.Timedelta(days=window_days)\n",
    "    history = df[(df['date'] >= start_date) & (df['date'] < date) & (df['node'] == node)]\n",
    "    context = \"\\n\".join(history['text'].tolist())\n",
    "    return context if context else \"No historical data.\"\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=150, window_days=3, days_ahead=1):\n",
    "        self.df = df.sort_values(by='date')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.window_days = window_days\n",
    "        self.days_ahead = days_ahead\n",
    "        self.dates = df['date'].unique()[window_days:]\n",
    "        self.nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "        print(f\"Dataset: {len(df['date'].unique())} unique dates, {len(self.dates)} dates after {window_days} days, {len(self.nodes)} nodes\")\n",
    "        if len(self.dates) == 0:\n",
    "            print(f\"Warning: No valid dates found after {window_days} days. Check dataset time range.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates) * len(self.nodes) * self.days_ahead\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day_idx = idx % self.days_ahead\n",
    "        node_idx = (idx // self.days_ahead) % len(self.nodes)\n",
    "        date_idx = idx // (len(self.nodes) * self.days_ahead)\n",
    "        date = pd.to_datetime(self.dates[date_idx])\n",
    "        node = self.nodes[node_idx]\n",
    "        future_date = date + pd.Timedelta(days=day_idx + 1)\n",
    "        \n",
    "        context = prepare_history_context(self.df, date, node)\n",
    "        future_data = self.df[(self.df['date'] == future_date) & (self.df['node'] == node)]\n",
    "        label = future_data['label'].iloc[0] if not future_data.empty else 0\n",
    "        \n",
    "        prompt = f\"Past 3 days:\\n{context}\\nPredict day {day_idx + 1} status: normal or abnormal\"\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        label_start = len(self.tokenizer.encode(prompt.rsplit(\":\", 1)[0] + \":\")) - 1\n",
    "        labels[:label_start] = -100\n",
    "        target_token = self.tokenizer.encode(\"normal\" if label == 0 else \"abnormal\", add_special_tokens=False)\n",
    "        target_token = target_token[-1] if target_token else self.tokenizer.pad_token_id\n",
    "        labels[label_start:] = target_token\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "train_dataset = TextDataset(train_text_df, tokenizer, window_days=3, days_ahead=1)\n",
    "test_dataset = TextDataset(test_text_df, tokenizer, window_days=3, days_ahead=1)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits[:, -1, :2], axis=-1)\n",
    "    labels = [label[-1] for label in labels if label[-1] in [0, 1]]\n",
    "    predictions = predictions[:len(labels)]\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, zero_division=0),\n",
    "        'precision': precision_score(labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(labels, predictions, zero_division=0)\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/qwen3-0.6B\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    warmup_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_accumulation_steps=16,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=custom_collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"models/best-qwen3-0.6B\")\n",
    "tokenizer.save_pretrained(\"models/best-qwen3-0.6B\")\n",
    "\n",
    "def predict_future_status(model, tokenizer, context, days_ahead=1, max_length=128):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    prompt = f\"Past 3 days:\\n{context}\\nPredict day 1 status: normal or abnormal\"\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    ).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            normal_token = tokenizer.encode(\"normal\", add_special_tokens=False)[-1]\n",
    "            abnormal_token = tokenizer.encode(\"abnormal\", add_special_tokens=False)[-1]\n",
    "            probs = torch.softmax(logits[:, [normal_token, abnormal_token]], dim=-1).cpu().numpy()[0]\n",
    "            pred = int(np.argmax(probs))\n",
    "            predictions.append(pred)\n",
    "            probabilities.append(probs[1])\n",
    "    del inputs, outputs, logits, probs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return predictions, probabilities\n",
    "\n",
    "test_dates = test_text_df['date'].unique()[3:]\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "predictions = []\n",
    "true_labels = []\n",
    "probabilities = []\n",
    "\n",
    "with tqdm(total=len(test_dates) * len(nodes), desc=\"Predicting future status\") as pbar:\n",
    "    for date in test_dates:\n",
    "        date = pd.to_datetime(date)\n",
    "        for node in nodes:\n",
    "            context = prepare_history_context(test_text_df, date, node)\n",
    "            if context != \"No historical data.\":\n",
    "                pred, prob = predict_future_status(model, tokenizer, context)\n",
    "                predictions.extend(pred)\n",
    "                probabilities.extend(prob)\n",
    "                future_date = date + pd.Timedelta(days=1)\n",
    "                future_data = test_text_df[(test_text_df['date'] == future_date) & (test_text_df['node'] == node)]\n",
    "                if not future_data.empty:\n",
    "                    true_labels.append(future_data['label'].iloc[0])\n",
    "                    print(f\"Node {node}, Day 1 (Date: {future_date}): Predicted={pred[0]}, Prob(abnormal)={prob[0]:.4f}, True={true_labels[-1]}\")\n",
    "                del pred, prob, future_data            \n",
    "            del context\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            pbar.update(1)\n",
    "\n",
    "min_len = min(len(predictions), len(true_labels))\n",
    "predictions = predictions[:min_len]\n",
    "true_labels = true_labels[:min_len]\n",
    "probabilities = probabilities[:min_len]\n",
    "\n",
    "qwen_metrics = {\n",
    "    'accuracy': accuracy_score(true_labels, predictions),\n",
    "    'f1': f1_score(true_labels, predictions, zero_division=0),\n",
    "    'precision': precision_score(true_labels, predictions, zero_division=0),\n",
    "    'recall': recall_score(true_labels, predictions, zero_division=0),\n",
    "    'confusion_matrix': confusion_matrix(true_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"True labels:\", pd.Series(true_labels).value_counts().to_dict())\n",
    "print(\"Predictions:\", pd.Series(predictions).value_counts().to_dict())\n",
    "print(\"Qwen3-0.6B Metrics:\")\n",
    "print(f\"Accuracy: {qwen_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1: {qwen_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {qwen_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {qwen_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {qwen_metrics['confusion_matrix']}\")\n",
    "print(f\"Average abnormal probability: {np.mean(probabilities):.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795e076",
   "metadata": {},
   "source": [
    "# Qwen3-1.7B + Fine-tunning + RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen3DevOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
