{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002430b5",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074e7890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Processing dates:   1%|          | 1/182 [00:44<2:14:17, 44.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing dates: 100%|██████████| 182/182 [2:04:02<00:00, 40.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num samples generated: 1274, Total num abnormalities: 244 , Num abnormality rate: 19.15%\n",
      "Total text samples generated: 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "model_name = 'Qwen/Qwen3-0.6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Example data\n",
    "template_data = {\n",
    "    \"date\": [f\"2024-05-{str(day).zfill(2)}\" for day in range(1, 32)],\n",
    "    \"hivenode01_memory_usage\": [0.383553, 0.418137, 0.492768, 0.565933, 0.563794, 0.530966, 0.542266, 0.49038, 0.495031, 0.563613, \n",
    "                                0.495269, 0.618042, 0.536223, 0.613919, 0.508449, 0.576679, 0.521551, 0.622268, 0.611154, 0.518589, \n",
    "                                0.654723, 0.529004, 0.634645, 0.537238, 0.534131, 0.536508, 0.543266, 0.575944, 0.53557, 0.611579, 0.551645],\n",
    "    \"hivenode01_cpu_load_5min\": [8.53, 12.07, 9.09, 8.55, 12.5, 7.98, 8.93, 7.85, 7.22, 8.98, \n",
    "                                 9.34, 9.6, 10.06, 8.68, 9.03, 8.08, 9.02, 7.89, 7.6, 11.22, \n",
    "                                 9.58, 9.53, 8.96, 10.89, 6.7, 9.73, 9.5, 10.5, 9.11, 8.75, 6.51],\n",
    "    \"hivenode01_cpu_load_10min\": [9.12, 9.94, 9.3, 8.8, 11.66, 9.5, 8.63, 8.67, 7.77, 8.18, \n",
    "                                  8.99, 8.14, 9.02, 8.62, 8.54, 8.71, 8.91, 8.47, 7.66, 9.91, \n",
    "                                  8.67, 8.72, 8.77, 9.19, 7.58, 9.44, 8.89, 9.83, 8.32, 8.91, 8.17],\n",
    "    \"hivenode01_cpu_load_15min\": [9.49, 9.07, 9.6, 7.92, 9.79, 9.62, 8.36, 9.32, 8.07, 7.91, \n",
    "                                  8.99, 8.12, 8.4, 8.46, 8.25, 8.73, 9.04, 8.54, 8.02, 9.58, \n",
    "                                  8.62, 8.84, 8.31, 8.68, 8.11, 9.01, 8.3, 8.95, 8.64, 8.83, 8.73],\n",
    "    \"hivenode02_memory_usage\": [0.342719, 0.374206, 0.404097, 0.427319, 0.435087, 0.43736, 0.437884, 0.438484, 0.447014, 0.450261, \n",
    "                                0.456589, 0.457304, 0.461043, 0.459707, 0.456962, 0.460743, 0.470204, 0.466149, 0.474047, 0.476227, \n",
    "                                0.477294, 0.479754, 0.482177, 0.471509, 0.475243, 0.47675, 0.487594, 0.490805, 0.492136, 0.500023, 0.502623],\n",
    "    \"hivenode02_cpu_load_5min\": [6.51, 8.39, 10.08, 12.8, 12.48, 10.14, 12.26, 8.85, 5.73, 9.06, \n",
    "                                 7.78, 7.96, 7.47, 6.82, 5.79, 7.28, 6.23, 7.19, 6.14, 7.79, \n",
    "                                 6.3, 6.57, 7.69, 6.68, 6, 6.59, 7.59, 9.85, 7.96, 9.24, 7.76],\n",
    "    \"hivenode02_cpu_load_10min\": [6.27, 8.1, 10.61, 11.74, 12.44, 10.47, 11.24, 9.63, 7.08, 8.45, \n",
    "                                  7.22, 6.18, 6.94, 7, 6.41, 7.03, 6.72, 6.58, 6.04, 8.39, \n",
    "                                  6.34, 6.72, 6.91, 6.78, 6.35, 6.98, 6.84, 9.35, 8.14, 9.07, 7.54],\n",
    "    \"hivenode02_cpu_load_15min\": [6.74, 7.63, 10.64, 10.35, 11.11, 10.6, 10.01, 9.69, 7.04, 8.02, \n",
    "                                  6.8, 6.12, 6.77, 7.13, 6.7, 7.07, 7.05, 6.9, 6.66, 8.08, \n",
    "                                  7, 7.39, 6.79, 7.14, 6.82, 7.05, 7.04, 8.52, 8, 8.46, 7.58],\n",
    "    \"hivenode03_memory_usage\": [0.348359, 0.375609, 0.40562, 0.428764, 0.439686, 0.441799, 0.445848, 0.442726, 0.455341, 0.451364, \n",
    "                                0.458707, 0.458583, 0.463021, 0.474306, 0.468443, 0.46892, 0.47674, 0.473441, 0.477693, 0.477164, \n",
    "                                0.478697, 0.480722, 0.4821, 0.486745, 0.489345, 0.488128, 0.493571, 0.493586, 0.494228, 0.502349, 0.503208],\n",
    "    \"hivenode03_cpu_load_5min\": [8.1, 9.61, 7.42, 8.05, 10.37, 7.72, 11.72, 7.67, 6.14, 9.78, \n",
    "                                 7.17, 7.31, 10.6, 6.07, 5.34, 8.05, 6.25, 6.13, 6.1, 8.68, \n",
    "                                 10.09, 7.21, 7.5, 7.04, 7.07, 8.59, 10.04, 7.41, 6.68, 12.27, 6.57],\n",
    "    \"hivenode03_cpu_load_10min\": [7.6, 8.99, 8.52, 8.7, 10.56, 8.82, 9.46, 7.53, 7.36, 8.75, \n",
    "                                  7.09, 7.13, 9.56, 6.75, 6.73, 7.91, 6.71, 6.66, 7.11, 8.29, \n",
    "                                  8.12, 7, 6.84, 7.52, 7.84, 8.83, 8.43, 8.22, 7.42, 8.92, 7.05],\n",
    "    \"hivenode03_cpu_load_15min\": [7.77, 8.52, 8.85, 8.12, 9.56, 9.05, 8.9, 8.19, 7.57, 8.34, \n",
    "                                  7.3, 7.36, 8.66, 7.17, 7.57, 7.68, 7.33, 7.05, 7.46, 8.48, \n",
    "                                  7.77, 7.61, 7.16, 7.84, 8.22, 8.38, 8.05, 8.11, 7.66, 8.29, 7.49],\n",
    "    \"hivenode04_memory_usage\": [0.406619, 0.419903, 0.433678, 0.441757, 0.452172, 0.455543, 0.4598, 0.460577, 0.459536, 0.461846, \n",
    "                                0.465693, 0.474606, 0.478739, 0.470002, 0.465657, 0.467247, 0.473648, 0.473736, 0.478283, 0.482861, \n",
    "                                0.483788, 0.488319, 0.489811, 0.485238, 0.493503, 0.487962, 0.496605, 0.49354, 0.490805, 0.485683, 0.487579],\n",
    "    \"hivenode04_cpu_load_5min\": [8, 10.23, 11.02, 7.19, 14.19, 9.73, 8, 7.59, 5.16, 7.4, \n",
    "                                 6.75, 11.49, 7.46, 6.14, 5.92, 8.56, 8.31, 5.56, 5.3, 8.13, \n",
    "                                 9.03, 7.09, 6.19, 9.83, 7.2, 6.55, 8.41, 8.59, 8.73, 7.54, 8.48],\n",
    "    \"hivenode04_cpu_load_10min\": [6.98, 9.55, 9.47, 6.98, 10.99, 9.18, 7.47, 8.12, 6.7, 7.94, \n",
    "                                  6.55, 8.76, 7.3, 6.44, 7.34, 8.53, 8.74, 5.8, 6.15, 7.7, \n",
    "                                  8.45, 7, 6.08, 8.91, 7.49, 6.32, 8.62, 8.64, 9.39, 7.11, 8.12],\n",
    "    \"hivenode04_cpu_load_15min\": [7.1, 8.69, 8.87, 7.09, 9.12, 8.57, 7.35, 8.13, 7.05, 7.85, \n",
    "                                  6.75, 7.59, 7.14, 6.73, 7.68, 8.18, 8.23, 6.5, 6.73, 7.81, \n",
    "                                  7.99, 7.58, 6.5, 8.22, 7.47, 6.78, 7.95, 8.17, 9.28, 7.25, 7.87],\n",
    "    \"hivenode05_memory_usage\": [0.351445, 0.383579, 0.411094, 0.438981, 0.445999, 0.448852, 0.449878, 0.451535, 0.458583, 0.462726, \n",
    "                                0.466429, 0.469966, 0.471406, 0.469826, 0.481328, 0.483001, 0.476077, 0.489852, 0.49036, 0.485502, \n",
    "                                0.491012, 0.492307, 0.493674, 0.49574, 0.50239, 0.502286, 0.507123, 0.507206, 0.507957, 0.515078, 0.519858],\n",
    "    \"hivenode05_cpu_load_5min\": [6.35, 10.45, 7.17, 7.47, 6.62, 7.21, 9.36, 7.46, 6.07, 7.37, \n",
    "                                 7.32, 7.54, 7.96, 7.12, 6.64, 7.61, 9.68, 7.25, 8.29, 9.44, \n",
    "                                 11.2, 7.9, 9.29, 8.73, 10.4, 6.47, 7.19, 11.26, 9.63, 7.66, 9.04],\n",
    "    \"hivenode05_cpu_load_10min\": [6.28, 8.91, 7.31, 7.82, 7.7, 7.96, 7.92, 7.33, 6.2, 8.48, \n",
    "                                  6.69, 7.32, 7.58, 7.44, 6.99, 7.93, 7.84, 7.75, 7.74, 8.82, \n",
    "                                  10.24, 7.76, 7.84, 8.24, 8.31, 7.4, 7.27, 9.23, 8.78, 7.81, 8.21],\n",
    "    \"hivenode05_cpu_load_15min\": [6.8, 8.32, 7.67, 7.49, 7.61, 8.17, 7.36, 7.52, 6.51, 7.85, \n",
    "                                  7.04, 7.15, 7.65, 7.48, 7.28, 7.67, 7.78, 8.08, 7.52, 8.79, \n",
    "                                  9.57, 7.87, 7.51, 8.07, 7.82, 7.67, 7.49, 8.74, 8.16, 8.06, 7.79],\n",
    "    \"hivenode06_memory_usage\": [0.36604, 0.269375, 0.307352, 0.364142, 0.173979, 0.139898, 0.192349, 0.218628, 0.207623, 0.156441, \n",
    "                                0.164164, 0.205858, 0.18286, 0.156721, 0.154286, 0.194161, 0.153307, 0.17226, 0.154831, 0.158268, \n",
    "                                0.206682, 0.174523, 0.213316, 0.193095, 0.21925, 0.20669, 0.219678, 0.257834, 0.30988, 0.199916, 0.160641],\n",
    "    \"hivenode06_cpu_load_5min\": [3.42, 3.33, 2.71, 2.62, 3.36, 3.65, 2.88, 3, 3.22, 3.28, \n",
    "                                 2.28, 3.02, 3.02, 2.89, 3.22, 3.55, 3.16, 3.36, 3.44, 2.8, \n",
    "                                 2.95, 2.28, 3.14, 2.43, 3.47, 2.78, 2.93, 3.51, 2.69, 2.55, 3.2],\n",
    "    \"hivenode06_cpu_load_10min\": [3.09, 3.3, 3.01, 2.83, 3.27, 3.3, 3.22, 2.9, 3.19, 3.34, \n",
    "                                  2.94, 3.04, 3.19, 3.11, 3.57, 3.37, 3.31, 3.19, 3.12, 3.04, \n",
    "                                  3.05, 2.67, 3.22, 2.84, 3.73, 3.18, 3.1, 3.21, 3.08, 3.09, 3.39],\n",
    "    \"hivenode06_cpu_load_15min\": [3.02, 3.19, 3.03, 2.87, 3.27, 3.17, 3.19, 2.94, 3.12, 3.12, \n",
    "                                  3.06, 3.01, 3.18, 3.38, 3.54, 3.31, 3.35, 3.09, 3.03, 3.12, \n",
    "                                  3.01, 2.76, 3.19, 2.87, 3.45, 3.13, 3.14, 3.13, 3.04, 3.22, 3.33],\n",
    "    \"hivenode07_memory_usage\": [0.381104, 0.141959, 0.15592, 0.144891, 0.183825, 0.221047, 0.255345, 0.270316, 0.25585, 0.255843, \n",
    "                                0.186189, 0.215183, 0.259902, 0.310836, 0.185046, 0.190949, 0.17576, 0.179073, 0.207616, 0.205337, \n",
    "                                0.248384, 0.240304, 0.201106, 0.180418, 0.189253, 0.195343, 0.205485, 0.193422, 0.213659, 0.200243, 0.198259],\n",
    "    \"hivenode07_cpu_load_5min\": [2.87, 2.82, 2.36, 2.04, 2.55, 2.15, 2.05, 2.06, 2.34, 2.16, \n",
    "                                 2.96, 2.44, 2.46, 2.43, 2.34, 2.76, 2.37, 2.58, 2.23, 2.6, \n",
    "                                 2.36, 2.35, 2.13, 2.58, 2.24, 2.66, 2.89, 2.58, 2.12, 3.02, 2.59],\n",
    "    \"hivenode07_cpu_load_10min\": [2.41, 2.74, 2.3, 2.23, 2.55, 2.46, 2.54, 2.23, 2.37, 2.36, \n",
    "                                  2.7, 2.6, 2.57, 2.32, 2.67, 2.5, 2.62, 2.64, 2.25, 2.46, \n",
    "                                  2.78, 2.58, 2.36, 2.64, 2.39, 2.79, 2.56, 2.81, 2.43, 2.74, 2.75],\n",
    "    \"hivenode07_cpu_load_15min\": [2.32, 2.75, 2.42, 2.36, 2.45, 2.48, 2.51, 2.27, 2.39, 2.43, \n",
    "                                  2.58, 2.56, 2.54, 2.48, 2.74, 2.52, 2.62, 2.59, 2.43, 2.47, \n",
    "                                  2.71, 2.67, 2.38, 2.56, 2.53, 2.64, 2.42, 2.84, 2.59, 2.66, 2.72]\n",
    "}\n",
    "template_df = pd.DataFrame(template_data)\n",
    "\n",
    "# Date range\n",
    "start_date = datetime(2024, 12, 1)\n",
    "end_date = datetime(2025, 5, 31)\n",
    "\n",
    "# List of nodes\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "# Days in a month\n",
    "def days_in_month(year, month):\n",
    "    if month == 2:\n",
    "        # Check for leap year\n",
    "        if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "            return 29\n",
    "        return 28\n",
    "    return 31 if month in [1, 3, 5, 7, 8, 10, 12] else 30\n",
    "\n",
    "current_date = start_date\n",
    "num_data = []\n",
    "text_data = []\n",
    "total_num_samples = 0\n",
    "total_text_samples = 0\n",
    "total_num_abnormalities = 0\n",
    "total_days = (end_date - start_date).days + 1\n",
    "\n",
    "for _ in tqdm(range(total_days), desc=\"Processing dates\"):\n",
    "    days = days_in_month(current_date.year, current_date.month)\n",
    "\n",
    "    # clear num_data at the start of each month\n",
    "    if not num_data or current_date.day == 1:\n",
    "        num_data, text_data = [], []\n",
    "\n",
    "    template_day_idx = (current_date.day - 1) % 31\n",
    "    template_row = template_df.iloc[template_day_idx]\n",
    "    row = {\"date\": current_date.strftime(\"%Y-%m-%d\")}\n",
    "    \n",
    "    for node in tqdm(nodes, desc=\"Processing nodes\", leave=False):\n",
    "        # Generate num data\n",
    "        memory_usage = template_row[f\"{node}_memory_usage\"] + np.random.normal(0, 0.05) #normal distribution random noise\n",
    "        memory_usage = max(0, min(1, memory_usage))  #[0,1]\n",
    "        \n",
    "        cpu_load_5min = template_row[f\"{node}_cpu_load_5min\"] + np.random.normal(0, 1)\n",
    "        cpu_load_10min = template_row[f\"{node}_cpu_load_10min\"] + np.random.normal(0, 1)\n",
    "        cpu_load_15min = template_row[f\"{node}_cpu_load_15min\"] + np.random.normal(0, 1)\n",
    "\n",
    "        cpu_load_5min = max(0, cpu_load_5min) # Ensure non-negative\n",
    "        cpu_load_10min = max(0, cpu_load_10min)\n",
    "        cpu_load_15min = max(0, cpu_load_15min)\n",
    "\n",
    "        # Add node data to row\n",
    "        row[node] = node \n",
    "        row[f\"{node}_memory_usage\"] = memory_usage\n",
    "        row[f\"{node}_cpu_load_5min\"] = cpu_load_5min\n",
    "        row[f\"{node}_cpu_load_10min\"] = cpu_load_10min\n",
    "        row[f\"{node}_cpu_load_15min\"] = cpu_load_15min\n",
    "        \n",
    "        # Check for abnormalities\n",
    "        if memory_usage > 0.7 or cpu_load_5min > 10 or cpu_load_10min > 10 or cpu_load_15min > 10:\n",
    "            status = \"abnormal\"\n",
    "            label = 1\n",
    "            total_num_abnormalities += 1\n",
    "        else:\n",
    "            status = \"normal\"\n",
    "            label = 0\n",
    "\n",
    "        total_num_samples += 1\n",
    "\n",
    "        # Generate text data\n",
    "        prompt = (\n",
    "            f\"You are Qwen3-0.6B, a language model for generating operation logs. \"\n",
    "            f\"Generate a single log entry with these requirements:\\n\"\n",
    "            f\"1. Retain all numerical values unchanged.\\n\"\n",
    "            f\"2. Use ISO 8601 timestamp (to the second).\\n\"\n",
    "            f\"3. Include fields: log level (INFO/WARN/ERROR), event type, node ID, memory_usage, cpu_load_5min, cpu_load_10min, cpu_load_15min, status.\\n\"\n",
    "            f\"4. For status=normal: emphasize stability and optimal performance. \"\n",
    "            f\"For status=abnormal: include alert severity, likely root cause, impact scope, and remediation steps.\\n\"\n",
    "            f\"5. Vary syntax (active/passive voice, questions, imperatives), use domain terms (OOM, jitter, throughput), \"\n",
    "            f\"and keep entry between 50-150 characters.\\n\"\n",
    "            f\"6. Output only the log entry, without repeating this prompt or adding commentary.\\n\\n\"\n",
    "            f\"Input Data:\\n\"\n",
    "            f\"  timestamp: {current_date.strftime('%Y-%m-%dT%H:%M:%SZ')}\\n\"\n",
    "            f\"  node: {node}\\n\"\n",
    "            f\"  memory_usage: {memory_usage:.4f}\\n\"\n",
    "            f\"  cpu_load_5min: {cpu_load_5min:.2f}\\n\"\n",
    "            f\"  cpu_load_10min: {cpu_load_10min:.2f}\\n\"\n",
    "            f\"  cpu_load_15min: {cpu_load_15min:.2f}\\n\"\n",
    "            f\"  status: {status}\\n\"\n",
    "        )\n",
    "\n",
    "        generated = generator(prompt, max_new_tokens=150, num_return_sequences=1, do_sample=True, temperature=0.9)\n",
    "        text = generated[0]['generated_text'].strip()\n",
    "        text_data.append({\n",
    "            \"date\": current_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"node\": node,\n",
    "            \"text\": text,\n",
    "        })\n",
    "        # Track total samples\n",
    "        total_text_samples += 1\n",
    "\n",
    "    num_data.append(row)\n",
    "        \n",
    "    if current_date.day == days or current_date == end_date:\n",
    "        # Save num data in month\n",
    "        month_df = pd.DataFrame(num_data)\n",
    "        columns = ['date']\n",
    "        for node in nodes:\n",
    "            columns.extend([\n",
    "                f\"{node}_memory_usage\",\n",
    "                f\"{node}_cpu_load_5min\",\n",
    "                f\"{node}_cpu_load_10min\",\n",
    "                f\"{node}_cpu_load_15min\"\n",
    "            ])\n",
    "        month_df = month_df[columns]\n",
    "        year_month = current_date.strftime(\"%Y-%m\")\n",
    "        os.makedirs('data/pre_train_num', exist_ok=True)\n",
    "        month_df.to_csv(os.path.join('data/pre_train_num', f'{year_month}.csv'), index=False)\n",
    "\n",
    "        # Save text data in month\n",
    "        os.makedirs('data/pre_train_text', exist_ok=True)\n",
    "        with open(f'data/pre_train_text/{year_month}.jsonl', 'w', encoding='utf-8') as f:\n",
    "            for record in text_data:\n",
    "                json.dump(record, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "        num_data, text_data = [], []\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "num_abnormalities_rate = total_num_abnormalities / total_num_samples\n",
    "\n",
    "print(f'Total num samples generated: {total_num_samples}, Total num abnormalities: {total_num_abnormalities} , Num abnormality rate: {num_abnormalities_rate:.2%}')\n",
    "print(f'Total text samples generated: {total_text_samples}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcea5a",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f48144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging num data: 100%|██████████| 6/6 [00:00<00:00, 752.03it/s]\n",
      "Merging text data: 100%|██████████| 6/6 [00:00<00:00, 260.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical data: train set 1015 samples, abnormalities 192, abnormality rate 18.92%\n",
      "Numerical data: test set 259 samples, abnormalities 52, abnormality rate 20.08%\n",
      "Text data: train set 1019 samples, test set 255 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Merge num data\n",
    "num_files = sorted([f for f in os.listdir('data/pre_train_num') if f.endswith('.csv')])\n",
    "all_num_data = []\n",
    "for file in tqdm(num_files, desc=\"Merging num data\"):\n",
    "    df = pd.read_csv(os.path.join('data/pre_train_num', file))\n",
    "    all_num_data.append(df)\n",
    "num_df = pd.concat(all_num_data, ignore_index=True)\n",
    "num_df['date'] = pd.to_datetime(num_df['date'], format='%Y-%m-%d')\n",
    "num_df = num_df.sort_values('date')\n",
    "num_df.to_csv('data/pre_train_num/num_202412-202505.csv', index=False)\n",
    "\n",
    "# Merge text data\n",
    "text_files = sorted([f for f in os.listdir('data/pre_train_text') if f.endswith('.jsonl')])\n",
    "all_text_data = []\n",
    "for file in tqdm(text_files, desc=\"Merging text data\"):\n",
    "    with open(os.path.join('data/pre_train_text', file), 'r', encoding='utf-8') as f:\n",
    "        records = [json.loads(line) for line in f]\n",
    "        all_text_data.extend(records)\n",
    "text_df = pd.DataFrame(all_text_data)\n",
    "text_df['date'] = pd.to_datetime(text_df['date'], format='%Y-%m-%d')\n",
    "text_df = text_df.sort_values('date')\n",
    "# Save text data as JSONL\n",
    "with open('data/pre_train_text/text_202412-202505.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for _, row in text_df.iterrows():\n",
    "        record = row.to_dict()\n",
    "        if isinstance(record['date'], pd.Timestamp):\n",
    "            record['date'] = record['date'].strftime('%Y-%m-%d')\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Text train-test split (80/20)\n",
    "split_idx = int(len(num_df) * 0.8)\n",
    "train_num_df = num_df.iloc[:split_idx]\n",
    "test_num_df = num_df.iloc[split_idx:]\n",
    "\n",
    "split_idx_text = int(len(text_df) * 0.8)\n",
    "train_text_df = text_df.iloc[:split_idx_text]\n",
    "test_text_df = text_df.iloc[split_idx_text:]\n",
    "\n",
    "\n",
    "# Save datasets as Parquet to reduce storage space\n",
    "os.makedirs('data/traindata', exist_ok=True)\n",
    "os.makedirs('data/testdata', exist_ok=True)\n",
    "train_num_df.to_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df.to_parquet('data/testdata/test_num.parquet')\n",
    "train_text_df.to_parquet('data/traindata/train_text.parquet')\n",
    "test_text_df.to_parquet('data/testdata/test_text.parquet')\n",
    "\n",
    "# Calculate abnormality for numerical data\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "def count_abnormalities(df):\n",
    "    total_samples = len(df) * len(nodes)\n",
    "    total_abnormal = 0\n",
    "    for node in nodes:\n",
    "        abnormal = ((df[f\"{node}_memory_usage\"] > 0.7) | \n",
    "                    (df[f\"{node}_cpu_load_5min\"] > 10) | \n",
    "                    (df[f\"{node}_cpu_load_10min\"] > 10) | \n",
    "                    (df[f\"{node}_cpu_load_15min\"] > 10)).sum()\n",
    "        total_abnormal += abnormal\n",
    "    return total_samples, total_abnormal\n",
    "\n",
    "# Numerical data statistics\n",
    "train_num_samples, train_num_abnormal = count_abnormalities(train_num_df)\n",
    "test_num_samples, test_num_abnormal = count_abnormalities(test_num_df)\n",
    "train_abn_rate = train_num_abnormal / train_num_samples \n",
    "test_abn_rate = test_num_abnormal / test_num_samples \n",
    "\n",
    "print(f\"Numerical data: train set {train_num_samples} samples, abnormalities {train_num_abnormal}, abnormality rate {train_abn_rate:.2%}\")\n",
    "print(f\"Numerical data: test set {test_num_samples} samples, abnormalities {test_num_abnormal}, abnormality rate {test_abn_rate:.2%}\")\n",
    "print(f\"Text data: train set {train_text_df.shape[0]} samples, test set {test_text_df.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219c555",
   "metadata": {},
   "source": [
    "# LLM & Fine-Tunning -- Qwen3-1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688ace63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 06:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.891824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.749900</td>\n",
       "      <td>13.465537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.749900</td>\n",
       "      <td>12.396853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.300800</td>\n",
       "      <td>9.575779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.300800</td>\n",
       "      <td>7.043048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting future status: 100%|██████████| 30/30 [01:16<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-1.5B Future Prediction Metrics:\n",
      "Accuracy: 88.10%\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Confusion Matrix: [[555, 75], [0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load text datasets\n",
    "train_text_df = pd.read_parquet('data/traindata/train_text.parquet')\n",
    "test_text_df = pd.read_parquet('data/testdata/test_text.parquet')\n",
    "\n",
    "# Extract status labels\n",
    "def extract_status(text):\n",
    "    match = re.search(r\"(normal|abnormal)\", text, re.IGNORECASE)\n",
    "    return 0 if match and match.group(0).lower() == \"normal\" else 1\n",
    "\n",
    "train_text_df['label'] = train_text_df['text'].apply(extract_status)\n",
    "test_text_df['label'] = test_text_df['text'].apply(extract_status)\n",
    "\n",
    "# Initialize Qwen2-1.5B\n",
    "model_name = 'Qwen/Qwen2-1.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Prepare dataset for classification\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        # Create sequence labels: same label for all tokens, -100 for padding\n",
    "        labels = torch.full_like(input_ids, label)\n",
    "        labels[attention_mask == 0] = -100  # Ignore padding tokens\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_dataset = TextDataset(train_text_df['text'].tolist(), train_text_df['label'].tolist(), tokenizer)\n",
    "test_dataset = TextDataset(test_text_df['text'].tolist(), test_text_df['label'].tolist(), tokenizer)\n",
    "\n",
    "# Training configuration for classification\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/qwen2-1.5B-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_accumulation_steps=10,\n",
    "    report_to=\"none\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Fine-tune classification model\n",
    "trainer.train()\n",
    "trainer.save_model(\"models/qwen2-1.5B-finetuned\")\n",
    "tokenizer.save_pretrained(\"models/qwen2-1.5B-finetuned\")\n",
    "\n",
    "# Prepare context for future prediction (time-series)\n",
    "def prepare_history_context(df, date, node, window_days=7):\n",
    "    start_date = pd.to_datetime(date) - pd.Timedelta(days=window_days)\n",
    "    history = df[(df['date'] >= start_date) & (df['date'] < date) & (df['node'] == node)]\n",
    "    context = \"\\n\".join(history['text'].tolist())\n",
    "    return context\n",
    "\n",
    "# Predict future 3 days\n",
    "def predict_future_status(model, tokenizer, context, days_ahead=3, max_length=512):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    for day in range(1, days_ahead + 1):\n",
    "        prompt = (\n",
    "            f\"Based on the operation logs from the past 7 days:\\n{context}\\n\"\n",
    "            f\"Predict the system status (0=normal, 1=abnormal) and abnormal probability for day {day} ahead.\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt').to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :2]\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        predictions.append(pred)\n",
    "        probabilities.append(probs[1])\n",
    "    return predictions, probabilities\n",
    "\n",
    "# Test set prediction for time-series\n",
    "# Assume test_text_df contains 'date' and 'node' columns\n",
    "test_dates = test_text_df['date'].unique()\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "predictions = []\n",
    "true_labels = []\n",
    "probabilities = []\n",
    "\n",
    "for date in tqdm(test_dates[7:], desc=\"Predicting future status\"):\n",
    "    date = pd.to_datetime(date)\n",
    "    for node in nodes:\n",
    "        context = prepare_history_context(test_text_df, date, node)\n",
    "        if context:\n",
    "            pred, prob = predict_future_status(model, tokenizer, context)\n",
    "            predictions.extend(pred)\n",
    "            probabilities.extend(prob)\n",
    "            future_days = [date + pd.Timedelta(days=i) for i in range(1, 4)]\n",
    "            for future_date in future_days:\n",
    "                future_data = test_text_df[(test_text_df['date'] == future_date) & (test_text_df['node'] == node)]\n",
    "                true_labels.append(future_data['label'].iloc[0] if not future_data.empty else 0)\n",
    "\n",
    "# Truncate to match lengths\n",
    "min_len = min(len(predictions), len(true_labels))\n",
    "predictions = predictions[:min_len]\n",
    "true_labels = true_labels[:min_len]\n",
    "\n",
    "# Evaluate metrics\n",
    "qwen_metrics = {\n",
    "    'accuracy': accuracy_score(true_labels, predictions),\n",
    "    'f1': f1_score(true_labels, predictions),\n",
    "    'precision': precision_score(true_labels, predictions),\n",
    "    'recall': recall_score(true_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(true_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"Qwen2-1.5B Future Prediction Metrics:\")\n",
    "print(f\"Accuracy: {qwen_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {qwen_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {qwen_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {qwen_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {qwen_metrics['confusion_matrix']}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795e076",
   "metadata": {},
   "source": [
    "# Qwen3-1.7B + Fine-tunning + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a7b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 06:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.888096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.747700</td>\n",
       "      <td>13.460326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.747700</td>\n",
       "      <td>12.378171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.278500</td>\n",
       "      <td>9.428842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.278500</td>\n",
       "      <td>6.882834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weish\\AppData\\Local\\Temp\\ipykernel_13948\\804056066.py:122: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "RAG Predicting: 100%|██████████| 255/255 [00:13<00:00, 18.70it/s]\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-1.5B with RAG Metrics:\n",
      "Accuracy: 100.00%\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Confusion Matrix: [[255]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting future status RAG: 100%|██████████| 30/30 [01:26<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-1.5B Future Prediction with RAG Metrics:\n",
      "Accuracy: 99.05%\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Confusion Matrix: [[624, 6], [0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "d:\\anaconda3\\envs\\Qwen3DevOps\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load and preprocess data\n",
    "# -------------------------------\n",
    "train_text_df = pd.read_parquet('data/traindata/train_text.parquet')\n",
    "test_text_df = pd.read_parquet('data/testdata/test_text.parquet')\n",
    "\n",
    "def extract_status(text):\n",
    "    match = re.search(r\"(normal|abnormal)\", text, re.IGNORECASE)\n",
    "    return 0 if match and match.group(0).lower() == \"normal\" else 1\n",
    "\n",
    "train_text_df['label'] = train_text_df['text'].apply(extract_status)\n",
    "test_text_df['label'] = test_text_df['text'].apply(extract_status)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Fine-tune Qwen2-1.5B with LoRA for classification\n",
    "# -------------------------------\n",
    "model_name = 'Qwen/Qwen2-1.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to('cuda')\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        labels = torch.full_like(input_ids, label)\n",
    "        labels[attention_mask == 0] = -100\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_dataset = TextDataset(train_text_df['text'].tolist(), train_text_df['label'].tolist(), tokenizer)\n",
    "test_dataset = TextDataset(test_text_df['text'].tolist(), test_text_df['label'].tolist(), tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/qwen2-1.5B-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_accumulation_steps=10,\n",
    "    report_to=\"none\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=custom_collate_fn\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"models/qwen2-1.5B-finetuned\")\n",
    "tokenizer.save_pretrained(\"models/qwen2-1.5B-finetuned\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Build FAISS index on train texts for RAG\n",
    "# -------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "texts = train_text_df['text'].tolist()\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. RAG-based classification prediction on test set\n",
    "# -------------------------------\n",
    "def rag_predict(model, tokenizer, texts, vectorstore, max_length=128, k=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for text in tqdm(texts, desc=\"RAG Predicting\"):\n",
    "        docs = vectorstore.similarity_search(text, k=k)\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        prompt = f\"Context: {context}\\nLog: {text}\\nPredict status (0=normal, 1=abnormal):\"\n",
    "        inputs = tokenizer(prompt, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt').to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :2]\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "rag_predictions = rag_predict(model, tokenizer, test_text_df['text'].tolist(), vectorstore)\n",
    "rag_metrics = {\n",
    "    'accuracy': accuracy_score(test_text_df['label'], rag_predictions),\n",
    "    'f1': f1_score(test_text_df['label'], rag_predictions),\n",
    "    'precision': precision_score(test_text_df['label'], rag_predictions),\n",
    "    'recall': recall_score(test_text_df['label'], rag_predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_text_df['label'], rag_predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"Qwen2-1.5B with RAG Metrics:\")\n",
    "print(f\"Accuracy: {rag_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {rag_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {rag_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {rag_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {rag_metrics['confusion_matrix']}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Prepare context for future time-series prediction\n",
    "# -------------------------------\n",
    "def prepare_history_context(df, date, node, window_days=7):\n",
    "    start_date = pd.to_datetime(date) - pd.Timedelta(days=window_days)\n",
    "    history = df[(df['date'] >= start_date) & (df['date'] < date) & (df['node'] == node)]\n",
    "    context = \"\\n\".join(history['text'].tolist())\n",
    "    return context\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Predict future 3 days with RAG-augmented prompts\n",
    "# -------------------------------\n",
    "def predict_future_status_rag(model, tokenizer, context, vectorstore, days_ahead=3, max_length=512, k=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    for day in range(1, days_ahead + 1):\n",
    "        # Retrieve top-k similar past logs\n",
    "        docs = vectorstore.similarity_search(context, k=k)\n",
    "        retrieved = \" \".join([doc.page_content for doc in docs])\n",
    "        prompt = (\n",
    "            f\"Retrieved: {retrieved}\\nPast 7-day logs:\\n{context}\\n\"\n",
    "            f\"Predict status (0=normal, 1=abnormal) and abnormal probability for day {day} ahead.\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt').to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :2]\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        predictions.append(pred)\n",
    "        probabilities.append(probs[1])\n",
    "        # Append predicted day text for next iteration context if needed\n",
    "    return predictions, probabilities\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Time-series RAG-based evaluation on test set\n",
    "# -------------------------------\n",
    "test_dates = test_text_df['date'].unique()\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "predictions_ts = []\n",
    "true_labels_ts = []\n",
    "probabilities_ts = []\n",
    "\n",
    "for date in tqdm(test_dates[7:], desc=\"Predicting future status RAG\"):\n",
    "    date = pd.to_datetime(date)\n",
    "    for node in nodes:\n",
    "        context = prepare_history_context(test_text_df, date, node)\n",
    "        if context:\n",
    "            pred, prob = predict_future_status_rag(model, tokenizer, context, vectorstore)\n",
    "            predictions_ts.extend(pred)\n",
    "            probabilities_ts.extend(prob)\n",
    "            future_days = [date + pd.Timedelta(days=i) for i in range(1, 4)]\n",
    "            for future_date in future_days:\n",
    "                future_data = test_text_df[(test_text_df['date'] == future_date) & (test_text_df['node'] == node)]\n",
    "                true_labels_ts.append(future_data['label'].iloc[0] if not future_data.empty else 0)\n",
    "\n",
    "min_len_ts = min(len(predictions_ts), len(true_labels_ts))\n",
    "predictions_ts = predictions_ts[:min_len_ts]\n",
    "true_labels_ts = true_labels_ts[:min_len_ts]\n",
    "\n",
    "rag_ts_metrics = {\n",
    "    'accuracy': accuracy_score(true_labels_ts, predictions_ts),\n",
    "    'f1': f1_score(true_labels_ts, predictions_ts),\n",
    "    'precision': precision_score(true_labels_ts, predictions_ts),\n",
    "    'recall': recall_score(true_labels_ts, predictions_ts),\n",
    "    'confusion_matrix': confusion_matrix(true_labels_ts, predictions_ts).tolist()\n",
    "}\n",
    "\n",
    "print(\"Qwen2-1.5B Future Prediction with RAG Metrics:\")\n",
    "print(f\"Accuracy: {rag_ts_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {rag_ts_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {rag_ts_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {rag_ts_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {rag_ts_metrics['confusion_matrix']}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff521d",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adad8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Metrics:\n",
      "Accuracy: 93.44%\n",
      "F1 Score: 0.8571\n",
      "Precision: 0.7612\n",
      "Recall: 0.9808\n",
      "Confusion Matrix: [[191, 16], [1, 51]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_df = pd.read_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df = pd.read_parquet('data/testdata/test_num.parquet')\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "def prepare_num_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            features.append([\n",
    "                row[f\"{node}_memory_usage\"],\n",
    "                row[f\"{node}_cpu_load_5min\"],\n",
    "                row[f\"{node}_cpu_load_10min\"],\n",
    "                row[f\"{node}_cpu_load_15min\"]\n",
    "            ])\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_num_labels(df):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            is_abnormal = (row[f\"{node}_memory_usage\"] > 0.7 or\n",
    "                           row[f\"{node}_cpu_load_5min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_10min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_15min\"] > 10)\n",
    "            labels.append(1 if is_abnormal else 0)\n",
    "    return labels\n",
    "\n",
    "train_num_features = prepare_num_features(train_num_df)\n",
    "test_num_features = prepare_num_features(test_num_df)\n",
    "train_num_labels = generate_num_labels(train_num_df)\n",
    "test_num_labels = generate_num_labels(test_num_df)\n",
    "\n",
    "svm = SVC(kernel='rbf', class_weight='balanced')\n",
    "svm.fit(train_num_features, train_num_labels)\n",
    "predictions = svm.predict(test_num_features)\n",
    "svm_metrics = {\n",
    "    'accuracy': accuracy_score(test_num_labels, predictions),\n",
    "    'f1': f1_score(test_num_labels, predictions),\n",
    "    'precision': precision_score(test_num_labels, predictions),\n",
    "    'recall': recall_score(test_num_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_num_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"SVM Metrics:\")\n",
    "print(f\"Accuracy: {svm_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {svm_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {svm_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {svm_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {svm_metrics['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4078bdc",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dabffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Metrics:\n",
      "Accuracy: 96.91%\n",
      "F1 Score: 0.9200\n",
      "Precision: 0.9583\n",
      "Recall: 0.8846\n",
      "Confusion Matrix: [[205, 2], [6, 46]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_df = pd.read_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df = pd.read_parquet('data/testdata/test_num.parquet')\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "def prepare_num_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            features.append([\n",
    "                row[f\"{node}_memory_usage\"],\n",
    "                row[f\"{node}_cpu_load_5min\"],\n",
    "                row[f\"{node}_cpu_load_10min\"],\n",
    "                row[f\"{node}_cpu_load_15min\"]\n",
    "            ])\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_num_labels(df):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            is_abnormal = (row[f\"{node}_memory_usage\"] > 0.7 or\n",
    "                           row[f\"{node}_cpu_load_5min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_10min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_15min\"] > 10)\n",
    "            labels.append(1 if is_abnormal else 0)\n",
    "    return labels\n",
    "\n",
    "train_num_features = prepare_num_features(train_num_df)\n",
    "test_num_features = prepare_num_features(test_num_df)\n",
    "train_num_labels = generate_num_labels(train_num_df)\n",
    "test_num_labels = generate_num_labels(test_num_df)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_num_features, train_num_labels)\n",
    "predictions = knn.predict(test_num_features)\n",
    "knn_metrics = {\n",
    "    'accuracy': accuracy_score(test_num_labels, predictions),\n",
    "    'f1': f1_score(test_num_labels, predictions),\n",
    "    'precision': precision_score(test_num_labels, predictions),\n",
    "    'recall': recall_score(test_num_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_num_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"KNN Metrics:\")\n",
    "print(f\"Accuracy: {knn_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {knn_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {knn_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {knn_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {knn_metrics['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540675f",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b52be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Metrics:\n",
      "Accuracy: 99.23%\n",
      "F1 Score: 0.9808\n",
      "Precision: 0.9808\n",
      "Recall: 0.9808\n",
      "Confusion Matrix: [[206, 1], [1, 51]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_df = pd.read_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df = pd.read_parquet('data/testdata/test_num.parquet')\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "def prepare_num_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            features.append([\n",
    "                row[f\"{node}_memory_usage\"],\n",
    "                row[f\"{node}_cpu_load_5min\"],\n",
    "                row[f\"{node}_cpu_load_10min\"],\n",
    "                row[f\"{node}_cpu_load_15min\"]\n",
    "            ])\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_num_labels(df):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            is_abnormal = (row[f\"{node}_memory_usage\"] > 0.7 or\n",
    "                           row[f\"{node}_cpu_load_5min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_10min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_15min\"] > 10)\n",
    "            labels.append(1 if is_abnormal else 0)\n",
    "    return labels\n",
    "\n",
    "train_num_features = prepare_num_features(train_num_df)\n",
    "test_num_features = prepare_num_features(test_num_df)\n",
    "train_num_labels = generate_num_labels(train_num_df)\n",
    "test_num_labels = generate_num_labels(test_num_df)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(train_num_features, train_num_labels)\n",
    "predictions = rf.predict(test_num_features)\n",
    "rf_metrics = {\n",
    "    'accuracy': accuracy_score(test_num_labels, predictions),\n",
    "    'f1': f1_score(test_num_labels, predictions),\n",
    "    'precision': precision_score(test_num_labels, predictions),\n",
    "    'recall': recall_score(test_num_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_num_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"RF Metrics:\")\n",
    "print(f\"Accuracy: {rf_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {rf_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {rf_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {rf_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {rf_metrics['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749509a1",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f761d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Metrics:\n",
      "Accuracy: 94.21%\n",
      "F1 Score: 0.8485\n",
      "Precision: 0.8936\n",
      "Recall: 0.8077\n",
      "Confusion Matrix: [[202, 5], [10, 42]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_df = pd.read_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df = pd.read_parquet('data/testdata/test_num.parquet')\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "def prepare_num_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            features.append([\n",
    "                row[f\"{node}_memory_usage\"],\n",
    "                row[f\"{node}_cpu_load_5min\"],\n",
    "                row[f\"{node}_cpu_load_10min\"],\n",
    "                row[f\"{node}_cpu_load_15min\"]\n",
    "            ])\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_num_labels(df):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            is_abnormal = (row[f\"{node}_memory_usage\"] > 0.7 or\n",
    "                           row[f\"{node}_cpu_load_5min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_10min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_15min\"] > 10)\n",
    "            labels.append(1 if is_abnormal else 0)\n",
    "    return labels\n",
    "\n",
    "train_num_features = prepare_num_features(train_num_df)\n",
    "test_num_features = prepare_num_features(test_num_df)\n",
    "train_num_labels = generate_num_labels(train_num_df)\n",
    "test_num_labels = generate_num_labels(test_num_df)\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
    "ann.fit(train_num_features, train_num_labels)\n",
    "predictions = ann.predict(test_num_features)\n",
    "ann_metrics = {\n",
    "    'accuracy': accuracy_score(test_num_labels, predictions),\n",
    "    'f1': f1_score(test_num_labels, predictions),\n",
    "    'precision': precision_score(test_num_labels, predictions),\n",
    "    'recall': recall_score(test_num_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_num_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"ANN Metrics:\")\n",
    "print(f\"Accuracy: {ann_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {ann_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {ann_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {ann_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {ann_metrics['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704a8d6",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9741ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Metrics:\n",
      "Accuracy: 99.23%\n",
      "F1 Score: 0.9808\n",
      "Precision: 0.9808\n",
      "Recall: 0.9808\n",
      "Confusion Matrix: [[206, 1], [1, 51]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_df = pd.read_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df = pd.read_parquet('data/testdata/test_num.parquet')\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "def prepare_num_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            features.append([\n",
    "                row[f\"{node}_memory_usage\"],\n",
    "                row[f\"{node}_cpu_load_5min\"],\n",
    "                row[f\"{node}_cpu_load_10min\"],\n",
    "                row[f\"{node}_cpu_load_15min\"]\n",
    "            ])\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_num_labels(df):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            is_abnormal = (row[f\"{node}_memory_usage\"] > 0.7 or\n",
    "                           row[f\"{node}_cpu_load_5min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_10min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_15min\"] > 10)\n",
    "            labels.append(1 if is_abnormal else 0)\n",
    "    return labels\n",
    "\n",
    "train_num_features = prepare_num_features(train_num_df)\n",
    "test_num_features = prepare_num_features(test_num_df)\n",
    "train_num_labels = generate_num_labels(train_num_df)\n",
    "test_num_labels = generate_num_labels(test_num_df)\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "dt.fit(train_num_features, train_num_labels)\n",
    "predictions = dt.predict(test_num_features)\n",
    "dt_metrics = {\n",
    "    'accuracy': accuracy_score(test_num_labels, predictions),\n",
    "    'f1': f1_score(test_num_labels, predictions),\n",
    "    'precision': precision_score(test_num_labels, predictions),\n",
    "    'recall': recall_score(test_num_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_num_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"DT Metrics:\")\n",
    "print(f\"Accuracy: {dt_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {dt_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {dt_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {dt_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {dt_metrics['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135dd8b",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e2ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Metrics:\n",
      "Accuracy: 92.28%\n",
      "F1 Score: 0.8361\n",
      "Precision: 0.7286\n",
      "Recall: 0.9808\n",
      "Confusion Matrix: [[188, 19], [1, 51]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_df = pd.read_parquet('data/traindata/train_num.parquet')\n",
    "test_num_df = pd.read_parquet('data/testdata/test_num.parquet')\n",
    "nodes = ['hivenode01', 'hivenode02', 'hivenode03', 'hivenode04', 'hivenode05', 'hivenode06', 'hivenode07']\n",
    "\n",
    "def prepare_num_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            features.append([\n",
    "                row[f\"{node}_memory_usage\"],\n",
    "                row[f\"{node}_cpu_load_5min\"],\n",
    "                row[f\"{node}_cpu_load_10min\"],\n",
    "                row[f\"{node}_cpu_load_15min\"]\n",
    "            ])\n",
    "    return np.array(features)\n",
    "\n",
    "def generate_num_labels(df):\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        for node in nodes:\n",
    "            is_abnormal = (row[f\"{node}_memory_usage\"] > 0.7 or\n",
    "                           row[f\"{node}_cpu_load_5min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_10min\"] > 10 or\n",
    "                           row[f\"{node}_cpu_load_15min\"] > 10)\n",
    "            labels.append(1 if is_abnormal else 0)\n",
    "    return labels\n",
    "\n",
    "train_num_features = prepare_num_features(train_num_df)\n",
    "test_num_features = prepare_num_features(test_num_df)\n",
    "train_num_labels = generate_num_labels(train_num_df)\n",
    "test_num_labels = generate_num_labels(test_num_df)\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(train_num_features, train_num_labels)\n",
    "predictions = lr.predict(test_num_features)\n",
    "lr_metrics = {\n",
    "    'accuracy': accuracy_score(test_num_labels, predictions),\n",
    "    'f1': f1_score(test_num_labels, predictions),\n",
    "    'precision': precision_score(test_num_labels, predictions),\n",
    "    'recall': recall_score(test_num_labels, predictions),\n",
    "    'confusion_matrix': confusion_matrix(test_num_labels, predictions).tolist()\n",
    "}\n",
    "\n",
    "print(\"LR Metrics:\")\n",
    "print(f\"Accuracy: {lr_metrics['accuracy']:.2%}\")\n",
    "print(f\"F1 Score: {lr_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {lr_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {lr_metrics['recall']:.4f}\")\n",
    "print(f\"Confusion Matrix: {lr_metrics['confusion_matrix']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen3DevOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
